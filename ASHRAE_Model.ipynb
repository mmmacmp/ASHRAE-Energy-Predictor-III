{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slR8ro_AkGEm"
   },
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xAxhRicpOGF0"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection  import train_test_split\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v7pvd3hkQ6l"
   },
   "source": [
    "# **Connecting Drive to google colab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kD-pBydFOXqW",
    "outputId": "236e88b2-8cfc-44b7-c730-f059264df57f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n"
     ]
    }
   ],
   "source": [
    "# Mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVHMwofRkW75"
   },
   "source": [
    "# **Downoloading Dataset from kaggle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMidvYhUOJxA",
    "outputId": "49991133-095f-4e7d-fc9f-dd5fe72e3a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
      "Downloading weather_test.csv.zip to /content\n",
      "  0% 0.00/2.53M [00:00<?, ?B/s]\n",
      "100% 2.53M/2.53M [00:00<00:00, 84.8MB/s]\n",
      "Downloading weather_train.csv.zip to /content\n",
      "  0% 0.00/1.27M [00:00<?, ?B/s]\n",
      "100% 1.27M/1.27M [00:00<00:00, 149MB/s]\n",
      "Downloading test.csv.zip to /content\n",
      " 98% 164M/167M [00:01<00:00, 103MB/s] \n",
      "100% 167M/167M [00:01<00:00, 96.6MB/s]\n",
      "Downloading train.csv.zip to /content\n",
      " 94% 113M/120M [00:02<00:00, 42.1MB/s] \n",
      "100% 120M/120M [00:02<00:00, 59.3MB/s]\n",
      "Downloading building_metadata.csv to /content\n",
      "  0% 0.00/44.5k [00:00<?, ?B/s]\n",
      "100% 44.5k/44.5k [00:00<00:00, 40.7MB/s]\n",
      "Downloading sample_submission.csv.zip to /content\n",
      " 80% 71.0M/88.4M [00:00<00:00, 88.9MB/s]\n",
      "100% 88.4M/88.4M [00:00<00:00, 101MB/s] \n",
      "Archive:  weather_train.csv.zip\n",
      "  inflating: weather_train.csv       \n",
      "\n",
      "Archive:  weather_test.csv.zip\n",
      "  inflating: weather_test.csv        \n",
      "\n",
      "Archive:  sample_submission.csv.zip\n",
      "  inflating: sample_submission.csv   \n",
      "\n",
      "Archive:  train.csv.zip\n",
      "  inflating: train.csv               \n",
      "\n",
      "Archive:  test.csv.zip\n",
      "  inflating: test.csv                \n",
      "\n",
      "5 archives were successfully processed.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp /gdrive/MyDrive/Kaggle/kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "# Downloading Dataset\n",
    "!kaggle competitions download -c ashrae-energy-prediction --force\n",
    "#unzipping the zip files and deleting the zip files\n",
    "!unzip \\*.zip  && rm *.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gm1KN7mGkagm"
   },
   "source": [
    "# **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bXMCXpkHPAeO"
   },
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "train = pd.read_csv('/content/train.csv')\n",
    "test = pd.read_csv('/content/test.csv')\n",
    "weather_train = pd.read_csv('/content/weather_train.csv')\n",
    "weather_test = pd.read_csv('/content/weather_test.csv')\n",
    "building_meta = pd.read_csv('/content/building_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VMHAXiXwPgQF"
   },
   "outputs": [],
   "source": [
    "## Function to reduce the DF size\n",
    "def reduce_memory_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "x7qasZ7fPhb2",
    "outputId": "82d28186-2379-47e3-9aa1-e263ac9662a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.03 Mb (60.3% reduction)\n",
      "Mem. usage decreased to  3.07 Mb (68.1% reduction)\n",
      "Mem. usage decreased to 289.19 Mb (53.1% reduction)\n",
      "Mem. usage decreased to  6.08 Mb (68.1% reduction)\n",
      "Mem. usage decreased to 596.49 Mb (53.1% reduction)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697595</th>\n",
       "      <td>41697595</td>\n",
       "      <td>1444</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-09 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697596</th>\n",
       "      <td>41697596</td>\n",
       "      <td>1445</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-09 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697597</th>\n",
       "      <td>41697597</td>\n",
       "      <td>1446</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-09 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697598</th>\n",
       "      <td>41697598</td>\n",
       "      <td>1447</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-09 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697599</th>\n",
       "      <td>41697599</td>\n",
       "      <td>1448</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-05-09 07:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  building_id  meter            timestamp\n",
       "0                0            0      0  2017-01-01 00:00:00\n",
       "1                1            1      0  2017-01-01 00:00:00\n",
       "2                2            2      0  2017-01-01 00:00:00\n",
       "3                3            3      0  2017-01-01 00:00:00\n",
       "4                4            4      0  2017-01-01 00:00:00\n",
       "...            ...          ...    ...                  ...\n",
       "41697595  41697595         1444      0  2018-05-09 07:00:00\n",
       "41697596  41697596         1445      0  2018-05-09 07:00:00\n",
       "41697597  41697597         1446      0  2018-05-09 07:00:00\n",
       "41697598  41697598         1447      0  2018-05-09 07:00:00\n",
       "41697599  41697599         1448      0  2018-05-09 07:00:00\n",
       "\n",
       "[41697600 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_memory_usage(building_meta)\n",
    "reduce_memory_usage(weather_train)\n",
    "reduce_memory_usage(train)\n",
    "reduce_memory_usage(weather_test)\n",
    "reduce_memory_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDCgrwEvOtdF",
    "outputId": "9f695e59-250a-4ca5-ff84-e3ba9aac216e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20216100, 16)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging tables\n",
    "BuildingTrain = building_meta.merge(train, left_on='building_id', right_on='building_id' , how='left')\n",
    "BuildingTest = building_meta.merge(test, left_on='building_id', right_on='building_id' , how='left')\n",
    "train_merged=BuildingTrain.merge(weather_train,left_on=['site_id','timestamp'],right_on=['site_id','timestamp'],how='left')\n",
    "test_merged = BuildingTest.merge(weather_test,left_on=['site_id','timestamp'],right_on=['site_id','timestamp'],how='left')\n",
    "train_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHkOwTwiP3p-",
    "outputId": "c7880618-0434-4ed5-f45b-f74db165021e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del test\n",
    "del train\n",
    "del building_meta\n",
    "del BuildingTest\n",
    "del BuildingTrain\n",
    "del weather_test\n",
    "del weather_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUQP8Xm5khGx"
   },
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xTxK2xXVQV1F"
   },
   "outputs": [],
   "source": [
    "# Remove unuseful columns as we see in the analysis year_built , floor count both have high percentage of missing data \n",
    "\n",
    "train_merged = train_merged.drop(columns=['year_built', 'floor_count', 'wind_direction', 'dew_temperature'])\n",
    "test_merged = test_merged.drop(columns=['year_built', 'floor_count','wind_direction', 'dew_temperature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uxeZ9O1dQWVW"
   },
   "outputs": [],
   "source": [
    "# Convert timestamp into month and a day which from analysis we found that they gave a good insights on meter reading\n",
    "\n",
    "train_merged ['timestamp'] =  pd.to_datetime(train_merged['timestamp'])\n",
    "test_merged ['timestamp'] =  pd.to_datetime(test_merged['timestamp'])\n",
    "train_merged['Month']=pd.DatetimeIndex(train_merged['timestamp']).month\n",
    "test_merged['Month']=pd.DatetimeIndex(test_merged['timestamp']).month\n",
    "train_merged['Day']=pd.DatetimeIndex(train_merged['timestamp']).day\n",
    "test_merged['Day']=pd.DatetimeIndex(test_merged['timestamp']).day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-1lRRbFlQjr9"
   },
   "outputs": [],
   "source": [
    "# Here we combine the values for different years and different hours a day so what we want here to convert dataframe into for each (meter,building_id,primary_use,month,day) to have value of (meter_reading,air_temperature,wind_speed,precip_depth_1_hr,cloud_coverage,square_feet)\n",
    "train_merged= train_merged.groupby(['meter',train_merged['building_id'],'primary_use',train_merged['Month'], train_merged['Day']]).agg({'meter_reading':'sum', 'air_temperature': 'mean', 'wind_speed': 'mean', 'precip_depth_1_hr': 'mean', 'cloud_coverage': 'mean', 'square_feet': 'mean'})\n",
    "train_merged = train_merged.reset_index()\n",
    "test_merged_1= test_merged.groupby(['row_id','meter',test_merged['building_id'],'primary_use',test_merged['Month'], test_merged['Day']]).agg({ 'air_temperature': 'mean', 'wind_speed': 'mean', 'precip_depth_1_hr': 'mean', 'cloud_coverage': 'mean', 'square_feet': 'mean'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sc2gDeC_Q0Vg"
   },
   "outputs": [],
   "source": [
    "# To fill the NA value, change the data type to float 32.\n",
    "\n",
    "train_merged['wind_speed'] = train_merged['wind_speed'].astype('float32')\n",
    "train_merged['air_temperature'] = train_merged['air_temperature'].astype('float32')\n",
    "train_merged['precip_depth_1_hr'] = train_merged['precip_depth_1_hr'].astype('float32')\n",
    "train_merged['cloud_coverage'] = train_merged['cloud_coverage'].astype('float32')\n",
    "test_merged['wind_speed'] = test_merged['wind_speed'].astype('float32')\n",
    "test_merged['air_temperature'] = test_merged['air_temperature'].astype('float32')\n",
    "test_merged['precip_depth_1_hr'] = test_merged['precip_depth_1_hr'].astype('float32')\n",
    "test_merged['cloud_coverage'] = test_merged['cloud_coverage'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2CtzL3KaQ2K7",
    "outputId": "284d6e5b-f59f-43ba-88a1-1e3f7885766b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "meter                0\n",
       "building_id          0\n",
       "primary_use          0\n",
       "Month                0\n",
       "Day                  0\n",
       "meter_reading        0\n",
       "air_temperature      0\n",
       "wind_speed           0\n",
       "precip_depth_1_hr    0\n",
       "cloud_coverage       0\n",
       "square_feet          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here the percentage of missing data is high so we use the forward and backward methods to introduce some variety in filling missing data\n",
    "train_merged['precip_depth_1_hr'].fillna(method='ffill', inplace = True)\n",
    "train_merged['cloud_coverage'].fillna(method='bfill', inplace = True)\n",
    "\n",
    "# While here the percentage of missing data is low so we use the mean instead.\n",
    "train_merged['wind_speed'].fillna(train_merged['wind_speed'].mean(), inplace=True)\n",
    "train_merged['air_temperature'].fillna(train_merged['air_temperature'].mean(), inplace=True)\n",
    "\n",
    "# Same for test data\n",
    "test_merged['precip_depth_1_hr'].fillna(method='ffill', inplace = True)\n",
    "test_merged['cloud_coverage'].fillna(method='bfill', inplace = True)\n",
    "test_merged['precip_depth_1_hr'].fillna(test_merged['precip_depth_1_hr'].mean(), inplace=True)\n",
    "test_merged['cloud_coverage'].fillna(test_merged['cloud_coverage'].mean(), inplace=True)\n",
    "\n",
    "test_merged['wind_speed'].fillna(test_merged['wind_speed'].mean(), inplace=True)\n",
    "test_merged['air_temperature'].fillna(test_merged['air_temperature'].mean(), inplace=True)\n",
    "\n",
    "# Here we are checking that there is no null values\n",
    "train_merged.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "70GRQbayRCpn"
   },
   "outputs": [],
   "source": [
    "# Here we are going to encode categorical variable which primary_use\n",
    "# label encoding \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train_merged_encoded = train_merged[:]\n",
    "test_merged_encoded = test_merged[:]\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_merged_encoded[\"primary_use\"] = le.fit_transform(train_merged_encoded[\"primary_use\"])\n",
    "test_merged_encoded[\"primary_use\"] = le.fit_transform(test_merged_encoded[\"primary_use\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nkhChbXCRK3f"
   },
   "outputs": [],
   "source": [
    "# Features and labels in X, y\n",
    "\n",
    "X = train_merged_encoded[['meter', 'building_id', 'primary_use', 'Month', 'Day','air_temperature', 'wind_speed', 'precip_depth_1_hr', 'cloud_coverage',\n",
    "       'square_feet']]\n",
    "y = train_merged_encoded['meter_reading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5JkdmSeoRL5N"
   },
   "outputs": [],
   "source": [
    "# Split into training-validation-test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state= 45)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train,y_train, test_size = 0.2, random_state= 45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "VJe7FMHFNr8m"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.callbacks import  EarlyStopping\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection  import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SwRdzcG8N1wt"
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "  return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oDDtFRamN3MW"
   },
   "outputs": [],
   "source": [
    "def build_model(input_dim=10,metrics=root_mean_squared_error,loss='mse', optimizer=\"rmsprop\",drop_rate=0.5):\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Dense(128, activation='relu',input_shape=(None,input_dim)))\n",
    "  model.add(Dropout(drop_rate))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Dense(256, activation='relu'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Dropout(drop_rate))\n",
    "  model.add(Dense(256, activation='relu'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Dropout(drop_rate))\n",
    "  model.add(Dense(256, activation='relu'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Dropout(drop_rate))\n",
    "  model.add(Dense(1))\n",
    "  model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2pbDY0zFN5IP"
   },
   "outputs": [],
   "source": [
    "def train(model,x_train,y_train,epochs=50,batch_size=500,verbose=1,validation_data=(x_val,y_val),callbacks =None):\n",
    "  x_train = x_train.values[:]\n",
    "  x_train= x_train.reshape((x_train.shape[0],1,x_train.shape[-1]))\n",
    "  y_train = np.log1p(y_train)\n",
    "  if validation_data != None:\n",
    "    x_val = validation_data[0].values[:]\n",
    "    x_val = x_val.reshape((x_val.shape[0],1,x_val.shape[-1]))\n",
    "    y_val = np.log1p(validation_data[-1])\n",
    "      \n",
    "  return model.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,verbose=verbose,validation_data=(x_val,y_val),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2XJWWiJDN6sI"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_root_mean_squared_error', min_delta=0.0001, patience=5, verbose=True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "K-6r2WYSN7-R"
   },
   "outputs": [],
   "source": [
    "model = build_model(input_dim=x_train.shape[-1],drop_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f_N1AZlN9fR",
    "outputId": "78e43bd5-2784-476f-9dc7-125c6c6f96e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, None, 128)         1408      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 128)         512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 256)         33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 256)         65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, None, 256)         65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, None, 1)           257       \n",
      "=================================================================\n",
      "Total params: 169,857\n",
      "Trainable params: 168,065\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCtQ3YyMN-9d",
    "outputId": "573a7908-d317-4b64-b951-db623da06255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1083/1083 [==============================] - 34s 12ms/step - loss: 18.9772 - root_mean_squared_error: 3.9739 - val_loss: 5.7314 - val_root_mean_squared_error: 2.3907\n",
      "Epoch 2/30\n",
      "1083/1083 [==============================] - 12s 11ms/step - loss: 6.1454 - root_mean_squared_error: 2.4753 - val_loss: 5.7417 - val_root_mean_squared_error: 2.3926\n",
      "Epoch 3/30\n",
      "1083/1083 [==============================] - 12s 11ms/step - loss: 5.8294 - root_mean_squared_error: 2.4108 - val_loss: 5.8328 - val_root_mean_squared_error: 2.4120\n",
      "Epoch 4/30\n",
      "1083/1083 [==============================] - 12s 11ms/step - loss: 5.7807 - root_mean_squared_error: 2.4012 - val_loss: 5.7181 - val_root_mean_squared_error: 2.3882\n",
      "Epoch 5/30\n",
      "1083/1083 [==============================] - 11s 10ms/step - loss: 5.7312 - root_mean_squared_error: 2.3906 - val_loss: 5.6913 - val_root_mean_squared_error: 2.3823\n",
      "Epoch 6/30\n",
      "1083/1083 [==============================] - 11s 10ms/step - loss: 5.6908 - root_mean_squared_error: 2.3826 - val_loss: 5.7609 - val_root_mean_squared_error: 2.3972\n",
      "Epoch 7/30\n",
      "1083/1083 [==============================] - 11s 10ms/step - loss: 5.6199 - root_mean_squared_error: 2.3676 - val_loss: 5.8370 - val_root_mean_squared_error: 2.4131\n",
      "Epoch 8/30\n",
      "1083/1083 [==============================] - 11s 10ms/step - loss: 5.6507 - root_mean_squared_error: 2.3739 - val_loss: 5.6265 - val_root_mean_squared_error: 2.3691\n",
      "Epoch 9/30\n",
      "1083/1083 [==============================] - 11s 11ms/step - loss: 5.6100 - root_mean_squared_error: 2.3650 - val_loss: 5.6203 - val_root_mean_squared_error: 2.3668\n",
      "Epoch 10/30\n",
      "1083/1083 [==============================] - 11s 10ms/step - loss: 5.5897 - root_mean_squared_error: 2.3609 - val_loss: 5.7091 - val_root_mean_squared_error: 2.3861\n",
      "Epoch 11/30\n",
      "1083/1083 [==============================] - 11s 10ms/step - loss: 5.5750 - root_mean_squared_error: 2.3579 - val_loss: 5.8343 - val_root_mean_squared_error: 2.4118\n",
      "Epoch 12/30\n",
      "1083/1083 [==============================] - 11s 10ms/step - loss: 5.5463 - root_mean_squared_error: 2.3515 - val_loss: 5.5633 - val_root_mean_squared_error: 2.3548\n",
      "Epoch 13/30\n",
      "1083/1083 [==============================] - 11s 10ms/step - loss: 5.5278 - root_mean_squared_error: 2.3478 - val_loss: 5.7342 - val_root_mean_squared_error: 2.3915\n",
      "Epoch 14/30\n",
      "1083/1083 [==============================] - 11s 10ms/step - loss: 5.4926 - root_mean_squared_error: 2.3399 - val_loss: 5.5860 - val_root_mean_squared_error: 2.3601\n",
      "Epoch 15/30\n",
      "1083/1083 [==============================] - 11s 10ms/step - loss: 5.4512 - root_mean_squared_error: 2.3313 - val_loss: 5.7302 - val_root_mean_squared_error: 2.3901\n",
      "Epoch 16/30\n",
      "1083/1083 [==============================] - 11s 10ms/step - loss: 5.4492 - root_mean_squared_error: 2.3310 - val_loss: 5.7358 - val_root_mean_squared_error: 2.3916\n",
      "Epoch 17/30\n",
      "1083/1083 [==============================] - 11s 11ms/step - loss: 5.4327 - root_mean_squared_error: 2.3271 - val_loss: 5.9436 - val_root_mean_squared_error: 2.4342\n",
      "Epoch 00017: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = train(model,x_train,y_train,epochs=30,batch_size=500,verbose=1,validation_data=(x_val,y_val), callbacks =[es]) # callbacks =[mc, es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "md0GhnFpTF1n",
    "outputId": "b742755d-0029-439c-d294-b1cd33247aa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "1322/1322 [==============================] - 6s 4ms/step - loss: 6.0367 - root_mean_squared_error: 2.4429\n",
      "test loss, test acc: [6.036705493927002, 2.442908525466919]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(x_test)\n",
    "x_test_inf = x_test.values[:]\n",
    "#print(x_test_inf.shape[0],x_test_inf.shape[-1])\n",
    "x_test_inf = x_test_inf.reshape((x_test_inf.shape[0],1,x_test_inf.shape[-1]))\n",
    "#print(x_test_inf)\n",
    "y_test_inf = np.log1p(y_test)\n",
    "\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(x_test_inf, y_test_inf, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "# print(\"Generate predictions for 3 samples\")\n",
    "# predictions = model.predict(x_test_inf)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ASHRAE_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
